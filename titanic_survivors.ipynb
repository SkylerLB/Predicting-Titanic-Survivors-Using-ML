{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e85f8ff1",
   "metadata": {},
   "source": [
    "### Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e8bc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5751d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/titanic/train.csv')\n",
    "test = pd.read_csv('data/titanic/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dfcf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class imbalance\n",
    "import seaborn as sns\n",
    "print(train['Survived'].value_counts())\n",
    "print(train['Survived'].value_counts(normalize=True))\n",
    "\n",
    "# 1. Class imbalance (Survival counts)\n",
    "sns.countplot(data=train, x='Survived')\n",
    "plt.title('Survived vs. Not Survived')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e318faa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Numerical features distributions\n",
    "sns.histplot(train['Age'])\n",
    "plt.title('Age Distribution')\n",
    "plt.show()\n",
    "\n",
    "sns.histplot(train['SibSp'])\n",
    "plt.title('Siblings/Spouses Aboard')\n",
    "plt.show()\n",
    "\n",
    "sns.histplot(train['Parch'])\n",
    "plt.title('Parents/Children Aboard')\n",
    "plt.show()\n",
    "\n",
    "sns.histplot(train['Fare'])\n",
    "plt.title('Fare Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204db821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Categorical features distributions\n",
    "sns.countplot(data=train, x='Pclass')\n",
    "plt.title('Passenger Class Distribution')\n",
    "plt.show()\n",
    "\n",
    "sns.countplot(data=train, x='Sex')\n",
    "plt.title('Sex Distribution')\n",
    "plt.show()\n",
    "\n",
    "sns.countplot(data=train, x='Embarked')\n",
    "plt.title('Port of Embarkation Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f688700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Survival rate by sex\n",
    "sns.barplot(data=train, x='Sex', y='Survived')\n",
    "plt.title('Survival Rate by Gender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cddbe51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Survival rate by passenger class\n",
    "sns.barplot(data=train, x='Pclass', y='Survived')\n",
    "plt.title('Survival Rate by Passenger Class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ee5213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Survival rate by age\n",
    "train['Age Group'] = pd.cut(train['Age'], bins=[0, 12, 18, 35, 60, 80], labels=['Child', 'Teen', 'Young Adult', 'Adult', 'Senior'])\n",
    "\n",
    "sns.barplot(data=train, x='Age Group', y='Survived')\n",
    "plt.title('Survival Rate by Age Group')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca88430",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec100f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined training & testing data sets to handle missing values later\n",
    "\n",
    "combined = pd.concat([train, test], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db842d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing value count per feature\n",
    "\n",
    "test.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207f2026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting titles (Characters after the comma & before the dot)\n",
    "\n",
    "combined['Title'] = combined['Name'].str.extract(r',\\s*([^\\.]+)\\.', expand=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9e483b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef86a8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define different categoric features\n",
    "categorical_features = ['Sex', 'Embarked', 'Cabin', 'Pclass', 'Title']\n",
    "\n",
    "# Categorical transfomer pipeline\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d742872e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric_features\n",
    "numeric_features = ['Age', 'Fare', 'SibSp', 'Parch']\n",
    "\n",
    "# Numeric transfomer pipeline\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7930726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing steps \n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_features),\n",
    "        ('num', numeric_transformer, numeric_features)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd9f291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropped the 'Age Group' column I added before to\n",
    "# visualize the survival rate by age\n",
    "\n",
    "combined.drop('Age Group', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faf13e7",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b46653",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb11d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the combined data set\n",
    "\n",
    "train_data = combined.iloc[:891].copy()\n",
    "test_data = combined.iloc[891:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ff559f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data.drop(['Survived', 'Name', 'Ticket'], axis=1)\n",
    "y = train_data['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb284d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e18d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used Pipeline since some columns have non numerical values\n",
    "# which couldn't be converted when I tried to fit my models\n",
    "\n",
    "classification_models = {\n",
    "        'Logistic Regression' : Pipeline(steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('classifier', LogisticRegression(max_iter=1000))\n",
    "        ]),\n",
    "        'RandomForestClassifier' : Pipeline(steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('classifier', RandomForestClassifier())\n",
    "        ]),\n",
    "        'XGBoost' : Pipeline(steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('classifier', XGBClassifier())\n",
    "        ])\n",
    "    }\n",
    "\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2576cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42) # To avoid randomness\n",
    "\n",
    "for mode_name, model in classification_models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    results[mode_name] = model.score(X_test, y_test)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb5d048",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results.values(),\n",
    "                          results.keys(),\n",
    "                          columns=['Accuracy'])\n",
    "results_df.plot.bar()\n",
    "\n",
    "# Notice that Logistic Regression scored the highest, with ~82% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acaf6ee",
   "metadata": {},
   "source": [
    "### Logistic Regression Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1af75d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression hyperparameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "log_reg_grid = {\"classifier__C\": np.logspace(-4, 4, 20),\n",
    "                \"classifier__solver\": [\"liblinear\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a96d32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning using GridSearchCV\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('estimator', preprocessor),\n",
    "    ('classifier', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "gs_log = GridSearchCV(estimator=pipe,\n",
    "             param_grid=log_reg_grid)\n",
    "\n",
    "gs_log.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d4b491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding best parameters for tuning\n",
    "\n",
    "gs_log.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023f4641",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_log.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b18411",
   "metadata": {},
   "source": [
    "### Model Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c1ead5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(solver='liblinear', C=29.763514416313132)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('estimator', preprocessor),\n",
    "    ('classifier', clf)\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8142b9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a440fd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f183911f",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dffcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_conf_mat(y_test, y_preds):\n",
    "    \"\"\"\n",
    "    Plots a confusion matrix using Seaborn's heatmap().\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(3, 3))\n",
    "    ax = sns.heatmap(confusion_matrix(y_test, y_preds),\n",
    "                     annot=True,\n",
    "                     cbar=False)\n",
    "    plt.xlabel(\"True label\")\n",
    "    plt.ylabel(\"Predicted label\")\n",
    "    \n",
    "    bottom, top = ax.get_ylim()\n",
    "    ax.set_ylim(bottom + 0.5, top - 0.5);\n",
    "    \n",
    "plot_conf_mat(y_test, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2331b6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c387193",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_test, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497ac539",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_test, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fe1485",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efe505a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validated Accuracy\n",
    "cross_val_acc = np.mean(cross_val_score(pipe,\n",
    "                                        X,\n",
    "                                        y,\n",
    "                                        scoring=\"accuracy\",\n",
    "                                        cv=5))\n",
    "\n",
    "cross_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaa19d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validated Precision\n",
    "cross_val_precision = np.mean(cross_val_score(pipe,\n",
    "                                              X,\n",
    "                                              y,\n",
    "                                              scoring=\"precision\",\n",
    "                                              cv=5))\n",
    "\n",
    "cross_val_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee33e2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validated Recall\n",
    "cross_val_recall = np.mean(cross_val_score(pipe,\n",
    "                                           X,\n",
    "                                           y,\n",
    "                                           scoring=\"recall\",\n",
    "                                           cv=5))\n",
    "\n",
    "cross_val_recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caa4947",
   "metadata": {},
   "source": [
    "### Final Predictions\n",
    "\n",
    "After preprocessing the data (handling missing values, encoding categorical variables, and extracting titles from names), we trained and evaluated several classification models:      Logistic Regression, Random Forest Classifier, XGBoost Classifier.\n",
    "\n",
    "All three models achieved similar accuracy:\n",
    "\n",
    "'Logistic Regression': 0.8212,\n",
    "'RandomForestClassifier': 0.8045,\n",
    "'XGBoost': 0.8045\n",
    "\n",
    "After hyperparameter tuning, Logistic Regression actually performed worse, dropping from 0.82 to 0.79."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb853bb",
   "metadata": {},
   "source": [
    "### Insights\n",
    "\n",
    "Feature engineering, especially extracting titles from names (like Mr., Mrs., Miss), had a positive impact on all models.\n",
    "\n",
    "Tuning isn't always beneficial. In this case, tuning Logistic Regression may have led to overfitting or pushed it into a suboptimal configuration."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
